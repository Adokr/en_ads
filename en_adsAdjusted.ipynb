{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "from re import *\n",
        "import networkx as nx\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import string"
      ],
      "metadata": {
        "id": "xQivqQZpoqc0"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {
        "id": "xtgJXHrQj_b1"
      },
      "outputs": [],
      "source": [
        "logger = logging.Logger('catch_all')\n",
        "\n",
        "remultword = compile('\\d+.\\d+')\n",
        "\n",
        "def read_graph_from_lines(graph_lines: list) -> nx.DiGraph:\n",
        "    di_graph = nx.DiGraph()\n",
        "    di_graph.add_node(0)\n",
        "    for line in graph_lines:\n",
        "        #print(line)\n",
        "        di_graph.add_node(\n",
        "            int(line.split('\\t')[0]),\n",
        "            token=line.split('\\t')[1],\n",
        "            lemma=line.split('\\t')[2].strip(),\n",
        "            upos=line.split('\\t')[3],\n",
        "            xpos=line.split('\\t')[4],\n",
        "            ufeats=line.split('\\t')[5],\n",
        "            misc=line.split('\\t')[9])\n",
        "        di_graph.add_edge(\n",
        "            int(line.split('\\t')[6]),\n",
        "            int(line.split('\\t')[0]),\n",
        "            ud_label=line.split('\\t')[7])\n",
        "    return di_graph\n",
        "\n",
        "\n",
        "def read_graph(conllu, howManyGraphs=3000):\n",
        "    digraphs = []\n",
        "    newdoc_ids = []\n",
        "    doc_sentences_count = []\n",
        "    last_sent_id = []\n",
        "    k = 0\n",
        "    try:\n",
        "        graph_lines = []\n",
        "        i = 0\n",
        "        j = 0\n",
        "        for line in conllu.readlines():\n",
        "            if line.strip():\n",
        "                if line.startswith('# '):\n",
        "                    if line.startswith('# newdoc id'):\n",
        "                        if last_sent_id != []:\n",
        "                            doc_sentences_count.append(int(last_sent_id[0].split('-')[1]))\n",
        "                        newdoc_ids.append(line.split()[4:][0])\n",
        "\n",
        "                    #print(line\n",
        "                    elif line.startswith('# sent_id'):\n",
        "                        last_sent_id = line.split()[3:]\n",
        "                        #print(line.split()[3:])\n",
        "                    pass\n",
        "                elif remultword.search(line.split()[0]):\n",
        "                    if 'multiword_tokens' not in locals():\n",
        "                        multiword_tokens = []\n",
        "                    multiword_tokens.append(line.strip())\n",
        "                else:\n",
        "                    graph_lines.append(line.strip())\n",
        "                    j += 1\n",
        "            else:\n",
        "                di_graph = read_graph_from_lines(graph_lines[k:k+j])\n",
        "                digraphs.append((di_graph, multiword_tokens if 'multiword_tokens' in locals() else []))\n",
        "                if 'multiword_tokens' in locals():\n",
        "                    del multiword_tokens\n",
        "                else:\n",
        "                    None\n",
        "                k += j\n",
        "                j = 0\n",
        "                i = i + 1\n",
        "            if i == howManyGraphs:\n",
        "                break\n",
        "        doc_sentences_count.append(int(last_sent_id[0].split('-')[1]))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(e, exc_info=True)\n",
        "        return []\n",
        "    return digraphs, newdoc_ids, doc_sentences_count\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        conllu = open('eng.erst.gum_dev.conllu', 'r', encoding=\"utf8\")\n",
        "    except IOError:\n",
        "        print(\"The input conllu file not found\")\n",
        "        sys.exit(1)\n",
        "    except IndexError:\n",
        "        print(\"python\", sys.argv[0], \"inputfile outputfile\")\n",
        "        sys.exit(1)\n",
        "    graphs, newdoc_ids, doc_sentences_count = read_graph(conllu, 150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0nSHyNO-zg4",
        "outputId": "aa06cd33-df55-4d11-db48-8c04bcfb82f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DET\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def getGovernor(graph, node):\n",
        "   return nx.predecessor(graph, 0)[node]\n",
        "\n",
        "\n",
        "print(graphs[12][0].nodes[1]['upos'])\n",
        "\n",
        "#for node in graphs[12][0].nodes(data={\"token\"}):\n",
        " # print(node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "id": "t5UZV_pX_LoD"
      },
      "outputs": [],
      "source": [
        "TAGS = { 'acl:relcl', 'root'}\n",
        "VERB_TAGS = {'parataxis', 'acl', 'acl:relcl', 'advcl'}\n",
        "DISCOURSE_MARKERS = {'and', 'despite', 'prior'}\n",
        "\n",
        "#NOT USED YET\n",
        "def checkForDiscourseMarkers(graph, token):\n",
        "    found = False\n",
        "    for successor in graph.successors(token):\n",
        "        if graph.nodes[successor]['lemma'] in DISCOURSE_MARKERS and getGovernor(graph, getGovernor(graph, getGovernor(graph, successor)[0])[0])[0] != token-1:\n",
        "            found = True\n",
        "            break\n",
        "    return found\n",
        "##\n",
        "\n",
        "\n",
        "def find_governors_from_graph(graph: nx.DiGraph, subroot=None) -> list:\n",
        "    def is_valid_governor(root, token):\n",
        "        upos = graph.nodes[token]['upos']\n",
        "        deprel = graph.edges[root, token]['ud_label']\n",
        "        root_upos = graph.nodes[root]['upos']\n",
        "        pron_type = graph.nodes[token]['ufeats']\n",
        "        #print(graph.nodes[token]['token'])\n",
        "        #1. ogólna zasada, ale z dość szczegółowym wyjątkiem\n",
        "        if deprel in TAGS:\n",
        "            if pron_type == \"PronType=Rel\" and root_upos == \"PRON\":\n",
        "                return False\n",
        "            #if deprel == \"acl:relcl\" and graph.edges[getGovernor(graph, root)[0], root][\"ud_label\"] == \"nmod\" and root_upos == \"PRON\":\n",
        "             #   return False\n",
        "            return True\n",
        "\n",
        "        #2. ogólna\n",
        "        elif upos == \"VERB\" and deprel in VERB_TAGS:\n",
        "            return True\n",
        "\n",
        "        #3. dość ogólna\n",
        "        elif upos == \"VERB\" and deprel == \"conj\" and graph.edges[getGovernor(graph, root)[0], root][\"ud_label\"] not in {\"xcomp\"}: #acl:\n",
        "            return True\n",
        "\n",
        "        #4. dość ogólna\n",
        "        elif upos == \"VERB\" and deprel == \"ccomp\":# and check_successors(graph, token, {\"SCONJ\"}, {\"mark\"})[0]:\n",
        "            return True\n",
        "        #5. ogólna\n",
        "        elif upos == \"ADJ\" and deprel in VERB_TAGS and root_upos not in {\"ADJ\", \"NOUN\", \"PRON\"}:\n",
        "            return True\n",
        "        #6. dość ogólna\n",
        "        elif upos == \"ADV\" and deprel == \"advcl:relcl\":# and check_successors(graph, token, {\"VERB\"}, {\"xcomp\", \"aux\"})[0]:\n",
        "            return True\n",
        "\n",
        "        #elif upos == \"VERB\" and deprel == \"advcl:relcl\" and graph.nodes[root]['ufeats'] == \"PronType=Rel\":\n",
        "         #   return True\n",
        "\n",
        "\n",
        "        #7. raczej szczegółowa\n",
        "        elif upos == \"ADV\" and deprel == \"obl\" and pron_type == \"PronType=Rel\" and graph.edges[getGovernor(graph, root)[0], root][\"ud_label\"] == \"xcomp\":\n",
        "            return True\n",
        "\n",
        "        #13. szczegółowa: wydzielanie nawiasów i ich zawartości\n",
        "        elif deprel == \"dep\" and check_successors(graph, token, {\"PUNCT\"}, {\"punct\"})[0]:\n",
        "             for successor in graph.successors(token):\n",
        "                if graph.nodes[successor]['lemma'] in {\"(\", \"{\"}:\n",
        "                    for successor2 in graph.successors(token):\n",
        "                        if graph.nodes[successor2]['lemma'] in {\")\", \"}\"}:\n",
        "                            return True\n",
        "        #14 ja bym wydzielał tylko jeśli wśród rodziców nie ma już tokenu oznaczonego jako gov\n",
        "        elif deprel == \"discourse\":\n",
        "            return True\n",
        "\n",
        "        #9. raczej szczegółowa\n",
        "\n",
        "        #elif upos == \"NOUN\" and root_upos != \"NOUN\" and check_successors(graph, token, {\"AUX\"}, {\"cop\"}) and check_successors(graph, token, {\"CCONJ\"}, {\"cc\"})[0]:\n",
        "         #   return True\n",
        "\n",
        "        '''\n",
        "        #8. raczej szczegółowa\n",
        "        elif upos == \"NOUN\" and deprel == \"appos\" and check_successors(graph, token, {\"PUNCT\"}, {\"punct\"})[0]:\n",
        "            return True'''\n",
        "\n",
        "\n",
        "\n",
        "        '''\n",
        "        #10. szczegółowa: liczby w nawiasach bo dużo ich było w tekście naukowym jako przypisy\n",
        "        elif upos == \"NUM\" and deprel in {\"nmod\", \"dep\"}:\n",
        "            first_gov = getGovernor(graph, token)[0]\n",
        "            if first_gov == 0:\n",
        "                return False\n",
        "            second_gov = getGovernor(graph, first_gov)[0]\n",
        "            second_deprel = graph.edges[second_gov, first_gov]['ud_label']\n",
        "            if second_deprel not in {\"nmod\", \"dep\", \"conj\", \"nummod\"}:\n",
        "                return True\n",
        "            elif graph.nodes[first_gov]['upos'] != \"NUM\":\n",
        "                return True\n",
        "        #elif upos == \"PROPN\" and deprel == \"conj\" and root_upos == \"PROPN\" and root != 0 and graph.edges[getGovernor(graph, root)[0], root][\"ud_label\"] == \"dep\":\n",
        "         #   return True\n",
        "        #11.\n",
        "        elif deprel == \"nmod\" and check_successors(graph, token, {\"VERB\"}, {\"case\"})[0]:\n",
        "            return True\n",
        "            '''#elif upos == \"SCONJ\" and deprel == \"mark\":\n",
        "            #gov_token = getGovernor(graph, token)[0]\n",
        "            #if token + 1 != gov_token:\n",
        "             #   return True\n",
        "        '''\n",
        "\n",
        "        #12. WYDZIELANIE FRAZ PRZYIKMOWYCH Z MARKERAMI DYSKURSU\n",
        "        elif deprel == \"obl\":\n",
        "           successor = check_successors(graph, token, {\"ADP\"}, {\"case\"})[0]\n",
        "           if successor and successor['lemma'] in DISCOURSE_MARKERS:\n",
        "                print(\"ZNALEZION\")\n",
        "                return True\n",
        "           else:\n",
        "                successor = check_successors(graph, token, {\"ADV\"}, {\"case\"})\n",
        "                if successor[1] == 0:\n",
        "                    return False\n",
        "                elif successor and check_successors(graph, successor[1], {\"ADP\"}, {\"fixed\"}) and successor[0]['lemma'] in DISCOURSE_MARKERS:\n",
        "                    return True\n",
        "        '''\n",
        "\n",
        "\n",
        "        return False\n",
        "\n",
        "    root = subroot if subroot is not None else list(graph.successors(0))[0]\n",
        "    governors = [root] if subroot is None else []\n",
        "\n",
        "    for token in graph.successors(root):\n",
        "        if is_valid_governor(root, token):\n",
        "            governors.append(token)\n",
        "        governors.extend(find_governors_from_graph(graph, token))\n",
        "\n",
        "\n",
        "    return governors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {
        "id": "qdVGdoKE_PeU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def check_successors(graph, token, target_upostags, target_deprels):\n",
        "    \"\"\"\n",
        "    Check if any successors of a given token match specific UPOS tags and dependency relations.\n",
        "    Returns the matching token's node attributes if found, otherwise False.\n",
        "    \"\"\"\n",
        "    for successor in graph.successors(token):\n",
        "        upos = graph.nodes[successor]['upos']\n",
        "        governor = getGovernor(graph, successor)[0]\n",
        "        deprel = graph.edges[governor, successor]['ud_label']\n",
        "\n",
        "        if upos in target_upostags and deprel in target_deprels:\n",
        "            return graph.nodes[successor], successor\n",
        "\n",
        "    return False, False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {
        "id": "ZoaKKNnTg_Lw"
      },
      "outputs": [],
      "source": [
        "def get_spans(graph: nx.DiGraph, span_heads: list, idx) -> list:\n",
        "    \"\"\"\n",
        "    Given a list of head nodes, return non-overlapping descendant spans from the graph.\n",
        "\n",
        "    Args:\n",
        "        graph: A NetworkX DiGraph representing the dependency structure.\n",
        "        span_heads: A list of node IDs considered as span heads.\n",
        "\n",
        "    Returns:\n",
        "        A list of sets, each representing a proper (non-overlapping) span.\n",
        "    \"\"\"\n",
        "    all_spans = []\n",
        "\n",
        "    # Collect full descendant spans including the head\n",
        "    for head in span_heads:\n",
        "        span = nx.descendants(graph, head)\n",
        "        span.add(head)\n",
        "        span &= set(graph.nodes)  # Ensure span only includes valid node IDs\n",
        "        all_spans.append(span)\n",
        "\n",
        "    # Sort spans by size (largest first)\n",
        "    all_spans.sort(key=len, reverse=True)\n",
        "\n",
        "    proper_spans = []\n",
        "    for i, current_span in enumerate(all_spans):\n",
        "        # Remove overlap with all smaller spans\n",
        "        for other_span in all_spans[i+1:]:\n",
        "            current_span -= other_span\n",
        "        if current_span:\n",
        "            proper_spans.append(current_span)\n",
        "\n",
        "    # Sort spans by their minimum node ID for consistency\n",
        "    proper_spans.sort(key=min)\n",
        "    if idx == GLOBAL:\n",
        "        print(proper_spans)\n",
        "    return proper_spans\n",
        "\n",
        "\n",
        "def check_continuity(my_list):\n",
        "    return all(a+1==b for a, b in zip(my_list, my_list[1:]))\n",
        "\n",
        "\n",
        "def findSplit(my_list):\n",
        "    splits = []\n",
        "    for a, b in zip(my_list, my_list[1:]):\n",
        "        if a+1!=b:\n",
        "            splits.append((a, b))\n",
        "    return splits\n",
        "\n",
        "def removeDiscourseFromGovs(graph, governors):\n",
        "    new_governors = []\n",
        "    for gov in governors:\n",
        "        if graph.edges[getGovernor(graph, gov)[0], gov]['ud_label'] != \"discourse\":\n",
        "            new_governors.append(gov)\n",
        "        elif getGovernor(graph, gov)[0] not in governors:\n",
        "            new_governors.append(gov)\n",
        "            print(\"I DO\")\n",
        "    return new_governors\n",
        "\n",
        "def prepare_doc(graph: nx.DiGraph, idx) -> list:\n",
        "    \"\"\"\n",
        "    Prepares a list of beginning token indices for continuous spans found in the graph.\n",
        "\n",
        "    Returns:\n",
        "        List of token indices marking the beginning of each proper span.\n",
        "    \"\"\"\n",
        "    governors = find_governors_from_graph(graph, None)\n",
        "    governors = removeDiscourseFromGovs(graph, governors)\n",
        "\n",
        "    if idx == GLOBAL:\n",
        "      print(f\"gov {sorted(governors)}\")\n",
        "    # Unpack both spans and discourse markers (if present)\n",
        "    token_spans = get_spans(graph, governors, idx)\n",
        "\n",
        "    proper_spans = []\n",
        "\n",
        "    for raw_span in token_spans:\n",
        "        sorted_span = sorted(raw_span)\n",
        "\n",
        "        if check_continuity(sorted_span):\n",
        "            proper_spans.append(sorted_span)\n",
        "        else:\n",
        "            splits = findSplit(sorted_span)\n",
        "\n",
        "            if splits:\n",
        "                for i, (start, end) in enumerate(splits):\n",
        "                    start_idx = sorted_span.index(start)\n",
        "                    end_idx = sorted_span.index(end)\n",
        "\n",
        "                    if i == 0:\n",
        "                        proper_spans.append(sorted_span[:start_idx + 1])\n",
        "                        if len(splits) == 1:\n",
        "                            proper_spans.append(sorted_span[end_idx:])\n",
        "                    elif i < len(splits) - 1:\n",
        "                        prev_end_idx = sorted_span.index(splits[i - 1][1])\n",
        "                        proper_spans.append(sorted_span[prev_end_idx:start_idx + 1])\n",
        "                    else:\n",
        "                        prev_end_idx = sorted_span.index(splits[i - 1][1])\n",
        "                        proper_spans.append(sorted_span[prev_end_idx:start_idx + 1])\n",
        "                        proper_spans.append(sorted_span[end_idx:])\n",
        "\n",
        "    # Remove invalid span if it accidentally includes a non-existent index\n",
        "    node_count = len(graph)\n",
        "    if [node_count] in proper_spans:\n",
        "        proper_spans.remove([node_count])\n",
        "\n",
        "    # Return the starting index of each proper span\n",
        "    span_beginnings = [span[0] for span in proper_spans]\n",
        "    return span_beginnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {
        "id": "iNeBOaZPlMRf"
      },
      "outputs": [],
      "source": [
        "def prepare_data(discourse_unit_starts: list, texts: list) -> list:\n",
        "    \"\"\"\n",
        "    Generates segmentation labels for each token in the input texts.\n",
        "\n",
        "    Args:\n",
        "        discourse_unit_starts: A list of lists, where each inner list contains indices\n",
        "                               marking the beginning of a discourse unit for a corresponding text.\n",
        "        texts: A list of tokenized texts (as lists of tokens or characters).\n",
        "\n",
        "    Returns:\n",
        "        A list of lists containing segmentation labels (\"Seg=B-seg\" or \"Seg=O\") for each token.\n",
        "    \"\"\"\n",
        "    eduLabels = []\n",
        "\n",
        "    for tokens, start_indices in zip(texts, discourse_unit_starts):\n",
        "        if tokens and not tokens[0].startswith(\"# \"):  # skip comment lines\n",
        "            labels = [\"Seg=O\"] * len(tokens)\n",
        "            for idx in start_indices:\n",
        "                if 0 <= idx - 1 < len(labels):\n",
        "                    labels[idx - 1] = \"Seg=B-seg\"\n",
        "            eduLabels.append(labels)\n",
        "        else:\n",
        "            eduLabels.append([None])\n",
        "    return eduLabels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load the input file and parse it into graphs\n",
        "    with open('eng.erst.gum_dev.conllu', 'r', encoding=\"utf8\") as conllu_file:\n",
        "        graphs, newdoc_ids, doc_sentences_count = read_graph(conllu_file)\n",
        "\n",
        "    token_lists = []\n",
        "    predicted_beginnings = []\n",
        "    #############################\n",
        "    if False:\n",
        "        for graph in graphs[idx:idx+1]:\n",
        "            for node in graph[0].nodes:\n",
        "                print(graph[0].nodes[node])\n",
        "\n",
        "\n",
        "\n",
        "    #########################\n",
        "    mwt_tab = []\n",
        "    ellipsis_tab = []\n",
        "    for i, (graph, mwt_lines) in enumerate(graphs):\n",
        "      # Parse MWT lines into a dict for quick lookup\n",
        "        # Prepare MWTs as a dictionary\n",
        "        mwt_dict = {}\n",
        "        ellipsis_dict = {}\n",
        "        for line in mwt_lines:\n",
        "            parts = line.split('\\t')\n",
        "            if '-' in parts[0]:\n",
        "                mwt_dict[int(parts[0].split('-')[0])] = (parts[0], parts[1])  # {893: (\"893-894\", \"that's\")}\n",
        "            elif '.' in parts[0]:\n",
        "                ellipsis_dict[int(parts[0].split('.')[0])] = (parts[0], parts[1])\n",
        "        mwt_tab.append(mwt_dict)\n",
        "        ellipsis_tab.append(ellipsis_dict)\n",
        "\n",
        "        tokens = [graph.nodes[node]['token'] for node in sorted(graph.nodes) if node != 0]\n",
        "        token_lists.append(tokens)\n",
        "        beginnings = prepare_doc(graph, i)\n",
        "        if i == GLOBAL:\n",
        "            print(f\"BEG {sorted(beginnings)}\")\n",
        "            print(tokens)\n",
        "\n",
        "\n",
        "        # Post-process beginnings to adjust for punctuation and edge cases\n",
        "        clean_beginnings = []\n",
        "        for idx in beginnings:\n",
        "            token = graph.nodes[idx]['token']\n",
        "            if token not in {\",\", \"’\", '\"', \"”\", \"“\", \":\", \";\", \")\"}:\n",
        "                clean_beginnings.append(idx)\n",
        "            elif idx + 1 < len(graph.nodes):  # Shift forward if punctuation\n",
        "                clean_beginnings.append(idx + 1)\n",
        "\n",
        "        # Remove final token as EDU if it's just the last token\n",
        "        if (len(graph.nodes) - 1) in clean_beginnings and len(graph.nodes) > 2:\n",
        "            clean_beginnings.remove(len(graph.nodes) - 1)\n",
        "\n",
        "        clean_beginnings = list(set(clean_beginnings))\n",
        "        if i == GLOBAL:\n",
        "            print(f\"CLEAN {sorted(clean_beginnings)}\")\n",
        "        predicted_beginnings.append(clean_beginnings)\n",
        "\n",
        "    # Create EDU segmentation tags\n",
        "    edu_labels = prepare_data(predicted_beginnings, token_lists)\n",
        "\n",
        "    # Write the output in tokenized format\n",
        "    with open('predicted.tok', \"w\", encoding=\"utf8\", newline=\"\") as out_file:\n",
        "        writer = csv.writer(out_file, delimiter=\"\\t\")\n",
        "        sent_index = 1\n",
        "\n",
        "        all_sentence_counter = 0\n",
        "\n",
        "        for doc_index, doc_id in enumerate(newdoc_ids):\n",
        "            token_idx = 1\n",
        "            if doc_index > 0:\n",
        "                writer.writerow([])  # Separate documents with a blank line\n",
        "\n",
        "            writer.writerow([f'# newdoc id = {doc_id}'])\n",
        "\n",
        "            # Get sentence span for current document\n",
        "            doc_start = sum(doc_sentences_count[:doc_index])\n",
        "            doc_end = doc_start + doc_sentences_count[doc_index]\n",
        "            doc_data = zip(token_lists[doc_start:doc_end], edu_labels[doc_start:doc_end])\n",
        "\n",
        "\n",
        "            for i, (token_seq, label_seq) in enumerate(doc_data):\n",
        "                token_in_sentence_counter = 1\n",
        "                for token, label in zip(token_seq, label_seq):\n",
        "\n",
        "\n",
        "                    # Check if this token is the start of a multi-word token\n",
        "                    if token_in_sentence_counter in mwt_tab[sent_index-1]:\n",
        "                        _, mwt_form = mwt_tab[sent_index-1][token_in_sentence_counter]\n",
        "                        writer.writerow([f\"{token_idx}-{token_idx+1}\", mwt_form] + ['_'] * 8)\n",
        "\n",
        "                    if token == '\"':\n",
        "                        out_file.write(f\"{token_idx}\\t{token}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t{label}\\n\")\n",
        "\n",
        "                    else:\n",
        "                        row = [token_idx, token, '_', '_', '_', '_', '_', '_', '_', label]\n",
        "                        writer.writerow(row)\n",
        "                    if token_in_sentence_counter in ellipsis_tab[sent_index-1]:\n",
        "                       elided_idx, elided_form = ellipsis_tab[sent_index-1][token_in_sentence_counter]\n",
        "                       writer.writerow([f\"{token_idx}.{elided_idx.split('.')[1]}\", elided_form] + ['_'] * 8)\n",
        "\n",
        "                    token_idx += 1\n",
        "                    token_in_sentence_counter += 1\n",
        "\n",
        "                sent_index += 1\n",
        "\n",
        "    #print(f\"lol: {[(i, tab) for (i, tab) in enumerate(mwt_tab) if tab != {}]}\")\n",
        "\n",
        "GLOBAL = 114\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrAWewp2sCII",
        "outputId": "d3d1a62b-13a5-478a-fd54-b1ecc9c02788"
      },
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gov [8, 10]\n",
            "[{1, 2, 3, 4, 5, 6, 7, 8, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47}, {9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}]\n",
            "BEG [1, 9, 25]\n",
            "['NOW', ',', 'THEREFORE', ',', 'we', 'do', 'hereby', 'Order', 'and', 'Direct', 'Major', '-', 'General', 'Scott', ',', 'the', 'Command', '-', 'in', '-', 'Chief', 'of', 'our', 'Armies', ',', 'immediately', 'upon', 'receipt', 'of', 'this', ',', 'our', 'Decree', ',', 'to', 'proceed', 'with', 'a', 'suitable', 'force', 'and', 'clear', 'the', 'Halls', 'of', 'Congress', '.']\n",
            "CLEAN [1, 9, 26]\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n",
            "I DO\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}