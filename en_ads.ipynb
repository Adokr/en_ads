{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "from re import *\n",
        "import networkx as nx\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import string"
      ],
      "metadata": {
        "id": "xQivqQZpoqc0"
      },
      "execution_count": 752,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 753,
      "metadata": {
        "id": "xtgJXHrQj_b1"
      },
      "outputs": [],
      "source": [
        "logger = logging.Logger('catch_all')\n",
        "\n",
        "remultword = compile('\\d+.\\d+')\n",
        "\n",
        "def read_graph_from_lines(graph_lines: list) -> nx.DiGraph:\n",
        "    di_graph = nx.DiGraph()\n",
        "    di_graph.add_node(0)\n",
        "    for line in graph_lines:\n",
        "        #print(line)\n",
        "        di_graph.add_node(\n",
        "            int(line.split('\\t')[0]),\n",
        "            token=line.split('\\t')[1],\n",
        "            lemma=line.split('\\t')[2].strip(),\n",
        "            upos=line.split('\\t')[3],\n",
        "            xpos=line.split('\\t')[4],\n",
        "            ufeats=line.split('\\t')[5],\n",
        "            misc=line.split('\\t')[9])\n",
        "        di_graph.add_edge(\n",
        "            int(line.split('\\t')[6]),\n",
        "            int(line.split('\\t')[0]),\n",
        "            ud_label=line.split('\\t')[7])\n",
        "    return di_graph\n",
        "\n",
        "\n",
        "def read_graph(conllu, howManyGraphs=3000):\n",
        "    digraphs = []\n",
        "    newdoc_ids = []\n",
        "    doc_sentences_count = []\n",
        "    last_sent_id = []\n",
        "    k = 0\n",
        "    try:\n",
        "        graph_lines = []\n",
        "        i = 0\n",
        "        j = 0\n",
        "        for line in conllu.readlines():\n",
        "            if line.strip():\n",
        "                if line.startswith('# '):\n",
        "                    if line.startswith('# newdoc id'):\n",
        "                        if last_sent_id != []:\n",
        "                            doc_sentences_count.append(int(last_sent_id[0].split('-')[1]))\n",
        "                        newdoc_ids.append(line.split()[4:][0])\n",
        "\n",
        "                    #print(line\n",
        "                    elif line.startswith('# sent_id'):\n",
        "                        last_sent_id = line.split()[3:]\n",
        "                        #print(line.split()[3:])\n",
        "                    pass\n",
        "                elif remultword.search(line.split()[0]):\n",
        "                    if 'multiword_tokens' not in locals():\n",
        "                        multiword_tokens = []\n",
        "                    multiword_tokens.append(line.strip())\n",
        "                else:\n",
        "                    graph_lines.append(line.strip())\n",
        "                    j += 1\n",
        "            else:\n",
        "                di_graph = read_graph_from_lines(graph_lines[k:k+j])\n",
        "                digraphs.append((di_graph, multiword_tokens if 'multiword_tokens' in locals() else []))\n",
        "                if 'multiword_tokens' in locals():\n",
        "                    del multiword_tokens\n",
        "                else:\n",
        "                    None\n",
        "                k += j\n",
        "                j = 0\n",
        "                i = i + 1\n",
        "            if i == howManyGraphs:\n",
        "                break\n",
        "        doc_sentences_count.append(int(last_sent_id[0].split('-')[1]))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(e, exc_info=True)\n",
        "        return []\n",
        "    return digraphs, newdoc_ids, doc_sentences_count\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        conllu = open('eng.erst.gum_dev.conllu', 'r', encoding=\"utf8\")\n",
        "    except IOError:\n",
        "        print(\"The input conllu file not found\")\n",
        "        sys.exit(1)\n",
        "    except IndexError:\n",
        "        print(\"python\", sys.argv[0], \"inputfile outputfile\")\n",
        "        sys.exit(1)\n",
        "    graphs, newdoc_ids, doc_sentences_count = read_graph(conllu, 150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 754,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0nSHyNO-zg4",
        "outputId": "026afa3b-c70d-4b6a-9e66-445f0cb35a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DET\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def getGovernor(graph, node):\n",
        "   return nx.predecessor(graph, 0)[node]\n",
        "\n",
        "\n",
        "print(graphs[12][0].nodes[1]['upos'])\n",
        "\n",
        "#for node in graphs[12][0].nodes(data={\"token\"}):\n",
        " # print(node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 755,
      "metadata": {
        "id": "t5UZV_pX_LoD"
      },
      "outputs": [],
      "source": [
        "TAGS = { 'acl:relcl', 'root'}\n",
        "VERB_TAGS = {'parataxis', 'acl', 'acl:relcl', 'advcl'}\n",
        "DISCOURSE_MARKERS = {'and', 'despite', 'prior'}\n",
        "\n",
        "#NOT USED YET\n",
        "def checkForDiscourseMarkers(graph, token):\n",
        "    found = False\n",
        "    for successor in graph.successors(token):\n",
        "        if graph.nodes[successor]['lemma'] in DISCOURSE_MARKERS and getGovernor(graph, getGovernor(graph, getGovernor(graph, successor)[0])[0])[0] != token-1:\n",
        "            found = True\n",
        "            break\n",
        "    return found\n",
        "##\n",
        "\n",
        "\n",
        "def find_governors_from_graph(graph: nx.DiGraph, subroot=None) -> list:\n",
        "    def is_valid_governor(root, token):\n",
        "        upos = graph.nodes[token]['upos']\n",
        "        deprel = graph.edges[root, token]['ud_label']\n",
        "        root_upos = graph.nodes[root]['upos']\n",
        "        ufeats = graph.nodes[token]['ufeats']\n",
        "        root_ufeats = graph.nodes[root]['ufeats']\n",
        "        #print(graph.nodes[token]['token'])\n",
        "\n",
        "        #1.\n",
        "        if deprel in TAGS:\n",
        "\n",
        "            #ex1. I know what you're doing. <- nie wydziela\n",
        "            #ex2. And what I would ask this court to do is to clarify,... <- nie wydzielam\n",
        "            if deprel == \"acl:relcl\" and root_ufeats == \"PronType=Rel\" and root_upos == \"PRON\":\n",
        "                return False\n",
        "            return True\n",
        "\n",
        "        #2.\n",
        "        elif upos == \"VERB\" and deprel in VERB_TAGS:\n",
        "            return True\n",
        "\n",
        "        #3.\n",
        "        elif upos == \"VERB\" and deprel == \"conj\" and graph.edges[getGovernor(graph, root)[0], root][\"ud_label\"] not in {\"xcomp\"}:\n",
        "            return True\n",
        "\n",
        "        #4. PYTANIE: Czy w tej zasadzie powinno się sprawdzać ten SCONJ/mark? Większa zgodność z dev jest bez sprawdzania i wtedy np wydziela trzecią i ostatnią sytuację: [Well you woke me up last night], [to tell me] [Kim wasn't spending the night].\n",
        "        elif upos == \"VERB\" and deprel == \"ccomp\":# and check_successors(graph, token, {\"SCONJ\"}, {\"mark\"})[0]:\n",
        "            return True\n",
        "\n",
        "        #6.\n",
        "        #Jedyne zdanie w dev, w którym ta reguła cokolwiek robi: Isn't it marvelous how wars, even when they are not fought, are found to betray everyone? Ale to i tak źle segmentuje, bo jest dziwne drzewo: \"wars\" jest dzieckiem \"found\"\n",
        "        elif upos == \"VERB\" and deprel == \"advcl:relcl\" and graph.nodes[root][\"ufeats\"] == \"PronType=Rel\" and check_successors(graph, token, {\"VERB\"}, {\"xcomp\", \"aux\"})[0]:\n",
        "            print(\"CHYBA NIC\")\n",
        "            return True\n",
        "\n",
        "        #5.\n",
        "        elif upos == \"ADJ\" and deprel in VERB_TAGS and root_upos not in {\"ADJ\", \"NOUN\", \"PRON\"}:\n",
        "            return True\n",
        "\n",
        "        #nowe\n",
        "        #ex1. And the second is that even on its own terms, it lacks internal coherence and is impermissibly vague, I say, and unworkable.  <- wydziela \"and is impermissibly vague... and unworkable\"\n",
        "        #ex2. There was no holder and the end was jagged. <- \"and the end was jagged\"\n",
        "        elif upos == \"ADJ\" and check_successors(graph, token, {\"AUX\"}, {\"cop\"})[0] and check_successors(graph, token, {\"CCONJ\"}, {\"cc\"})[0]:\n",
        "            return True\n",
        "\n",
        "        #NOWE\n",
        "        #ex1. And what I wish to emphasize is that this court has made it abundantly clear that careless behavior alone is not wrongful. <- wydziela w tym zdaniu \"that careless behaviour alone is not wrongful\", czyli tak jak jest w danych, ale nie wiem czy to ok\n",
        "        elif upos == \"ADJ\" and check_successors(graph, token, {\"AUX\"}, {\"cop\"})[0] and check_successors(graph, token, {\"SCONJ\"}, {\"mark\"}) and root_upos not in {\"ADJ\", \"NOUN\", \"PRON\"}:\n",
        "            return True\n",
        "\n",
        "        #7.\n",
        "        elif upos == \"ADV\" and deprel == \"obl\" and ufeats == \"PronType=Rel\" and graph.edges[getGovernor(graph, root)[0], root][\"ud_label\"] == \"xcomp\":\n",
        "            return True\n",
        "\n",
        "        #NOWE\n",
        "        #ex1. Such a scenario may be found in different situations, including when one studies a language in a classroom and then stops taking classes ...\n",
        "        #ex2. But how am I supposed to know when you're telling the truth?\n",
        "        #ex3. Isn't it marvelous how wars, even when they are not fought, are found to betray everyone? W tym zdaniu ta reguła wydziela sytuacje(how wars ... are found to betray everyone), która nie jest wydzielona w danych:\n",
        "        elif upos == \"ADV\" and ufeats == \"PronType=Rel\" and check_successors(graph, token, {\"VERB\", \"ADJ\", \"ADV\", \"NOUN\", \"AUX\"}, {\"advcl:relcl\"})[0]:\n",
        "            return True\n",
        "\n",
        "        #NOWE\n",
        "        #Desk research about what being a DH librarian entails\n",
        "        #elif upos == \"PRON\" and ufeats == \"PronType=Rel\" and check_successors(graph, token, {\"VERB\", \"NOUN\"}, {\"acl:relcl\"})[0]:\n",
        "         #   return True\n",
        "\n",
        "        #10. wydzielanie nawiasów i ich zawartości\n",
        "        #ex1. Research on adult-learned second language (L2) has provided considerable insight into the neurocognitive mechanisms underlying the learning and processing of L2 grammar [1]–[11].\n",
        "        #ex2. In fact, substantial periods (months to years) of limited or no exposure following L2 training are not uncommon, and may even be the norm [16].\n",
        "        #ex3. Higher levels of proficiency (or exposure) may be associated with less attrition [17], [18], [21], [23] or even with no observed losses [21].\n",
        "        #SPRAWDZANIE -LRB- -RRB-\n",
        "        elif deprel == \"dep\" and check_successors(graph, token, {\"PUNCT\"}, {\"punct\"})[0]:\n",
        "             for successor in graph.successors(token):\n",
        "                if graph.nodes[successor]['lemma'] in {\"(\", \"{\", \"[\"}:\n",
        "                    for successor2 in graph.successors(token):\n",
        "                        if graph.nodes[successor2]['lemma'] in {\")\", \"}\", \"]\"}:\n",
        "                            return True\n",
        "\n",
        "        #9.\n",
        "        #chyba nie powinno być root != NOUN (wtedy też zgodne ze zbiorem dev); tylko dwa zdania się różnią (w zależności czy jest root_upos != \"NOUN\" czy nie):\n",
        "        #ex1. General, is this a waiver, or is it a modification?\n",
        "        #ex2. To a few of us here today this is a solemn and most momentous occasion, and yet in the history of our nation it is a commonplace occurrence.\n",
        "        elif upos == \"NOUN\" and check_successors(graph, token, {\"AUX\"}, {\"cop\"})[0] and check_successors(graph, token, {\"CCONJ\"}, {\"cc\"})[0]:\n",
        "            return True\n",
        "\n",
        "        #14\n",
        "        #ex1. Yeah, exactly.\n",
        "        #ex2. Okay cool, cause she had to go bowling in the morning.\n",
        "        elif deprel == \"discourse\":\n",
        "            return True\n",
        "\n",
        "        '''\n",
        "        #12. WYDZIELANIE FRAZ PRZYIKMOWYCH Z MARKERAMI DYSKURSU\n",
        "        elif deprel == \"obl\":\n",
        "           successor = check_successors(graph, token, {\"ADP\"}, {\"case\"})[0]\n",
        "           if successor and successor['lemma'] in DISCOURSE_MARKERS:\n",
        "                print(\"ZNALEZION\")\n",
        "                return True\n",
        "           else:\n",
        "                successor = check_successors(graph, token, {\"ADV\"}, {\"case\"})\n",
        "                if successor[1] == 0:\n",
        "                    return False\n",
        "                elif successor and check_successors(graph, successor[1], {\"ADP\"}, {\"fixed\"}) and successor[0]['lemma'] in DISCOURSE_MARKERS:\n",
        "                    return True\n",
        "        '''\n",
        "\n",
        "\n",
        "        return False\n",
        "\n",
        "    root = subroot if subroot is not None else list(graph.successors(0))[0]\n",
        "    governors = [root] if subroot is None else []\n",
        "\n",
        "    for token in graph.successors(root):\n",
        "        if is_valid_governor(root, token):\n",
        "            governors.append(token)\n",
        "        governors.extend(find_governors_from_graph(graph, token))\n",
        "\n",
        "\n",
        "    return governors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 756,
      "metadata": {
        "id": "qdVGdoKE_PeU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def check_successors(graph, token, target_upostags, target_deprels):\n",
        "    \"\"\"\n",
        "    Check if any successors of a given token match specific UPOS tags and dependency relations.\n",
        "    Returns the matching token's node attributes if found, otherwise False.\n",
        "    \"\"\"\n",
        "    for successor in graph.successors(token):\n",
        "        upos = graph.nodes[successor]['upos']\n",
        "        governor = getGovernor(graph, successor)[0]\n",
        "        deprel = graph.edges[governor, successor]['ud_label']\n",
        "\n",
        "        if upos in target_upostags and deprel in target_deprels:\n",
        "            return graph.nodes[successor], successor\n",
        "\n",
        "    return False, False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 757,
      "metadata": {
        "id": "ZoaKKNnTg_Lw"
      },
      "outputs": [],
      "source": [
        "def get_spans(graph: nx.DiGraph, span_heads: list, idx) -> list:\n",
        "    \"\"\"\n",
        "    Given a list of head nodes, return non-overlapping descendant spans from the graph.\n",
        "\n",
        "    Args:\n",
        "        graph: A NetworkX DiGraph representing the dependency structure.\n",
        "        span_heads: A list of node IDs considered as span heads.\n",
        "\n",
        "    Returns:\n",
        "        A list of sets, each representing a proper (non-overlapping) span.\n",
        "    \"\"\"\n",
        "    all_spans = []\n",
        "\n",
        "    # Collect full descendant spans including the head\n",
        "    for head in span_heads:\n",
        "        span = nx.descendants(graph, head)\n",
        "        span.add(head)\n",
        "        span &= set(graph.nodes)  # Ensure span only includes valid node IDs\n",
        "        all_spans.append(span)\n",
        "\n",
        "    # Sort spans by size (largest first)\n",
        "    all_spans.sort(key=len, reverse=True)\n",
        "\n",
        "    proper_spans = []\n",
        "    for i, current_span in enumerate(all_spans):\n",
        "        # Remove overlap with all smaller spans\n",
        "        for other_span in all_spans[i+1:]:\n",
        "            current_span -= other_span\n",
        "        if current_span:\n",
        "            proper_spans.append(current_span)\n",
        "\n",
        "    # Sort spans by their minimum node ID for consistency\n",
        "    proper_spans.sort(key=min)\n",
        "    if idx == GLOBAL:\n",
        "        print(proper_spans)\n",
        "    return proper_spans\n",
        "\n",
        "\n",
        "def check_continuity(my_list):\n",
        "    return all(a+1==b for a, b in zip(my_list, my_list[1:]))\n",
        "\n",
        "\n",
        "def findSplit(my_list):\n",
        "    splits = []\n",
        "    for a, b in zip(my_list, my_list[1:]):\n",
        "        if a+1!=b:\n",
        "            splits.append((a, b))\n",
        "    return splits\n",
        "\n",
        "\n",
        "def prepare_doc(graph: nx.DiGraph, idx) -> list:\n",
        "    \"\"\"\n",
        "    Prepares a list of beginning token indices for continuous spans found in the graph.\n",
        "\n",
        "    Returns:\n",
        "        List of token indices marking the beginning of each proper span.\n",
        "    \"\"\"\n",
        "    governors = find_governors_from_graph(graph, None)\n",
        "\n",
        "    if idx == GLOBAL:\n",
        "      print(f\"gov {sorted(governors)}\")\n",
        "    # Unpack both spans and discourse markers (if present)\n",
        "    token_spans = get_spans(graph, governors, idx)\n",
        "\n",
        "    proper_spans = []\n",
        "\n",
        "    for raw_span in token_spans:\n",
        "        sorted_span = sorted(raw_span)\n",
        "\n",
        "        if check_continuity(sorted_span):\n",
        "            proper_spans.append(sorted_span)\n",
        "        else:\n",
        "            splits = findSplit(sorted_span)\n",
        "\n",
        "            if splits:\n",
        "                for i, (start, end) in enumerate(splits):\n",
        "                    start_idx = sorted_span.index(start)\n",
        "                    end_idx = sorted_span.index(end)\n",
        "\n",
        "                    if i == 0:\n",
        "                        proper_spans.append(sorted_span[:start_idx + 1])\n",
        "                        if len(splits) == 1:\n",
        "                            proper_spans.append(sorted_span[end_idx:])\n",
        "                    elif i < len(splits) - 1:\n",
        "                        prev_end_idx = sorted_span.index(splits[i - 1][1])\n",
        "                        proper_spans.append(sorted_span[prev_end_idx:start_idx + 1])\n",
        "                    else:\n",
        "                        prev_end_idx = sorted_span.index(splits[i - 1][1])\n",
        "                        proper_spans.append(sorted_span[prev_end_idx:start_idx + 1])\n",
        "                        proper_spans.append(sorted_span[end_idx:])\n",
        "\n",
        "    # Remove invalid span if it accidentally includes a non-existent index\n",
        "    node_count = len(graph)\n",
        "    if [node_count] in proper_spans:\n",
        "        proper_spans.remove([node_count])\n",
        "\n",
        "    # Return the starting index of each proper span\n",
        "    span_beginnings = [span[0] for span in proper_spans]\n",
        "    return span_beginnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 758,
      "metadata": {
        "id": "iNeBOaZPlMRf"
      },
      "outputs": [],
      "source": [
        "def prepare_data(discourse_unit_starts: list, texts: list) -> list:\n",
        "    \"\"\"\n",
        "    Generates segmentation labels for each token in the input texts.\n",
        "\n",
        "    Args:\n",
        "        discourse_unit_starts: A list of lists, where each inner list contains indices\n",
        "                               marking the beginning of a discourse unit for a corresponding text.\n",
        "        texts: A list of tokenized texts (as lists of tokens or characters).\n",
        "\n",
        "    Returns:\n",
        "        A list of lists containing segmentation labels (\"Seg=B-seg\" or \"Seg=O\") for each token.\n",
        "    \"\"\"\n",
        "    eduLabels = []\n",
        "\n",
        "    for tokens, start_indices in zip(texts, discourse_unit_starts):\n",
        "        if tokens and not tokens[0].startswith(\"# \"):  # skip comment lines\n",
        "            labels = [\"Seg=O\"] * len(tokens)\n",
        "            for idx in start_indices:\n",
        "                if 0 <= idx - 1 < len(labels):\n",
        "                    labels[idx - 1] = \"Seg=B-seg\"\n",
        "            eduLabels.append(labels)\n",
        "        else:\n",
        "            eduLabels.append([None])\n",
        "    return eduLabels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load the input file and parse it into graphs\n",
        "    with open('eng.erst.gum_dev.conllu', 'r', encoding=\"utf8\") as conllu_file:\n",
        "        graphs, newdoc_ids, doc_sentences_count = read_graph(conllu_file)\n",
        "\n",
        "    token_lists = []\n",
        "    predicted_beginnings = []\n",
        "    #############################\n",
        "    if False:\n",
        "        for graph in graphs[idx:idx+1]:\n",
        "            for node in graph[0].nodes:\n",
        "                print(graph[0].nodes[node])\n",
        "\n",
        "\n",
        "\n",
        "    #########################\n",
        "    mwt_tab = []\n",
        "    ellipsis_tab = []\n",
        "    for i, (graph, mwt_lines) in enumerate(graphs):\n",
        "      # Parse MWT lines into a dict for quick lookup\n",
        "        # Prepare MWTs as a dictionary\n",
        "        mwt_dict = {}\n",
        "        ellipsis_dict = {}\n",
        "        for line in mwt_lines:\n",
        "            parts = line.split('\\t')\n",
        "            if '-' in parts[0]:\n",
        "                mwt_dict[int(parts[0].split('-')[0])] = (parts[0], parts[1])  # {893: (\"893-894\", \"that's\")}\n",
        "            elif '.' in parts[0]:\n",
        "                ellipsis_dict[int(parts[0].split('.')[0])] = (parts[0], parts[1])\n",
        "        mwt_tab.append(mwt_dict)\n",
        "        ellipsis_tab.append(ellipsis_dict)\n",
        "\n",
        "        tokens = [graph.nodes[node]['token'] for node in sorted(graph.nodes) if node != 0]\n",
        "        token_lists.append(tokens)\n",
        "        beginnings = prepare_doc(graph, i)\n",
        "        if i == GLOBAL:\n",
        "            print(f\"BEG {sorted(beginnings)}\")\n",
        "            print(tokens)\n",
        "\n",
        "\n",
        "        # Post-process beginnings to adjust for punctuation and edge cases\n",
        "        clean_beginnings = []\n",
        "        for idx in beginnings:\n",
        "            token = graph.nodes[idx]['token']\n",
        "            if token not in {\",\", \"’\", '\"', \"”\", \"“\", \":\", \";\", \")\"}:\n",
        "                clean_beginnings.append(idx)\n",
        "            elif idx + 1 < len(graph.nodes):  # Shift forward if punctuation\n",
        "                clean_beginnings.append(idx + 1)\n",
        "\n",
        "        # Remove final token as EDU if it's just the last token\n",
        "        if (len(graph.nodes) - 1) in clean_beginnings and len(graph.nodes) > 2:\n",
        "            clean_beginnings.remove(len(graph.nodes) - 1)\n",
        "\n",
        "        clean_beginnings = list(set(clean_beginnings))\n",
        "        if i == GLOBAL:\n",
        "            print(f\"CLEAN {sorted(clean_beginnings)}\")\n",
        "        predicted_beginnings.append(clean_beginnings)\n",
        "\n",
        "    # Create EDU segmentation tags\n",
        "    edu_labels = prepare_data(predicted_beginnings, token_lists)\n",
        "\n",
        "    # Write the output in tokenized format\n",
        "    with open('predicted.tok', \"w\", encoding=\"utf8\", newline=\"\") as out_file:\n",
        "        writer = csv.writer(out_file, delimiter=\"\\t\")\n",
        "        sent_index = 1\n",
        "\n",
        "        all_sentence_counter = 0\n",
        "\n",
        "        for doc_index, doc_id in enumerate(newdoc_ids):\n",
        "            token_idx = 1\n",
        "            if doc_index > 0:\n",
        "                writer.writerow([])  # Separate documents with a blank line\n",
        "\n",
        "            writer.writerow([f'# newdoc id = {doc_id}'])\n",
        "\n",
        "            # Get sentence span for current document\n",
        "            doc_start = sum(doc_sentences_count[:doc_index])\n",
        "            doc_end = doc_start + doc_sentences_count[doc_index]\n",
        "            doc_data = zip(token_lists[doc_start:doc_end], edu_labels[doc_start:doc_end])\n",
        "\n",
        "\n",
        "            for i, (token_seq, label_seq) in enumerate(doc_data):\n",
        "                token_in_sentence_counter = 1\n",
        "                for token, label in zip(token_seq, label_seq):\n",
        "\n",
        "\n",
        "                    # Check if this token is the start of a multi-word token\n",
        "                    if token_in_sentence_counter in mwt_tab[sent_index-1]:\n",
        "                        _, mwt_form = mwt_tab[sent_index-1][token_in_sentence_counter]\n",
        "                        writer.writerow([f\"{token_idx}-{token_idx+1}\", mwt_form] + ['_'] * 8)\n",
        "\n",
        "                    if token == '\"':\n",
        "                        out_file.write(f\"{token_idx}\\t{token}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t{label}\\n\")\n",
        "\n",
        "                    else:\n",
        "                        row = [token_idx, token, '_', '_', '_', '_', '_', '_', '_', label]\n",
        "                        writer.writerow(row)\n",
        "                    if token_in_sentence_counter in ellipsis_tab[sent_index-1]:\n",
        "                       elided_idx, elided_form = ellipsis_tab[sent_index-1][token_in_sentence_counter]\n",
        "                       writer.writerow([f\"{token_idx}.{elided_idx.split('.')[1]}\", elided_form] + ['_'] * 8)\n",
        "\n",
        "                    token_idx += 1\n",
        "                    token_in_sentence_counter += 1\n",
        "\n",
        "                sent_index += 1\n",
        "\n",
        "    #print(f\"lol: {[(i, tab) for (i, tab) in enumerate(mwt_tab) if tab != {}]}\")\n",
        "\n",
        "GLOBAL = 167\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrAWewp2sCII",
        "outputId": "dcd7b04c-dccd-465e-b735-9d08897b68dc"
      },
      "execution_count": 759,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gov [2, 10, 15, 26]\n",
            "[{1, 2, 3, 27}, {4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {14, 15, 16, 17, 18}, {19, 20, 21, 22, 23, 24, 25, 26}]\n",
            "BEG [1, 4, 14, 19, 27]\n",
            "['You', 'know', 'Kendra', ',', 'y-', '—', 'I', 'do', \"n't\", 'know', 'how', 'many', 'times', 'I', 'got', 'ta', 'tell', 'you', ',', 'once', 'you', 'lie', ',', 'once', 'you', 'lie', '—']\n",
            "CLEAN [1, 5, 14, 20]\n",
            "CHYBA NIC\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}